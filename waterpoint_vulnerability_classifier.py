"""
Waterpoint Vulnerability Classifier
====================================
Uses the Anthropic API to classify WPdx+ waterpoints in Burkina Faso
by their flood vulnerability (Phase 1 — flood framework only).

Pipeline:
  1. Fetch waterpoint data from WPdx+ Socrata API (Burkina Faso)
  2. For each waterpoint, call Claude to classify flood risk
  3. Output a scored CSV ready for mapping

Requirements:
  pip install anthropic requests pandas tqdm

Usage:
  export ANTHROPIC_API_KEY=your_key_here
  python waterpoint_vulnerability_classifier.py

  Optional flags:
    --limit 100        # number of waterpoints to process (default: 500)
    --output results.csv
    --country "Burkina Faso"
"""

import os
import json
import time
import argparse
import requests
import pandas as pd
from tqdm import tqdm
import anthropic

# ── Configuration ────────────────────────────────────────────────────────────

WPDX_API_URL = "https://data.waterpointdata.org/resource/eqje-vguj.json"

# Districts covered by the Bangladesh AA Flood Framework 2025
# Source: OCHA Bangladesh Monsoon Floods AA Framework, April 2025
# WPdx+ uses district names in the adm2 field for Bangladesh
# Two river basins covered:
# Note: Only Gaibandha and Kurigram have WPdx+ data available
# Other framework districts (Bogra, Jamalpur, Sirajganj, Padma basin) 
# are not in the WPdx+ dataset for Bangladesh
JAMUNA_DISTRICTS = {
    "Gaibandha",
    "Kurigram",
}

PADMA_DISTRICTS = set()  # No data available yet

# All framework districts with available data
FLOOD_DISTRICTS = JAMUNA_DISTRICTS | PADMA_DISTRICTS

# For region filtering - Bangladesh adm1 divisions containing target districts
# Jamuna districts span Rajshahi and Mymensingh divisions
# Padma districts are in Dhaka division
FLOOD_REGIONS = {
    "Rajshahi",    # Bogra, Sirajganj, Gaibandha (partly)
    "Rangpur",     # Gaibandha, Kurigram
    "Mymensingh",  # Jamalpur
    "Dhaka",       # Madaripur, Manikganj, Shariatpur
}

TARGET_REGIONS = FLOOD_REGIONS

# Columns we send to Claude - chosen for vulnerability relevance
VULNERABILITY_COLUMNS = [
    "water_source_clean",  # clean water source type
    "water_tech_clean",    # clean technology type
    "status_clean",        # Functional / Non-Functional / Unknown
    "install_year",
    "distance_to_primary", # remoteness / access difficulty
    "local_population",    # population served
    "subjective_quality",  # perceived water quality
    "facility_type",       # Improved / Unimproved
    "is_urban",            # urban vs rural context
    "elevation_m",         # elevation in metres (from Open-Meteo Copernicus DEM)
]

# Columns we keep in output for mapping/reference
ID_COLUMNS = [
    "wpdx_id",
    "lat_deg",
    "lon_deg",
    "clean_country_name",
    "clean_adm1",         # division
    "clean_adm2",         # district
    "clean_adm3",         # upazila
]

# ── System prompt ─────────────────────────────────────────────────────────────

SYSTEM_PROMPT = """You are an expert WASH (Water, Sanitation and Hygiene) engineer embedded 
with OCHA's Anticipatory Action team in Bangladesh. You have deep field knowledge of how 
monsoon river floods damage rural water infrastructure in Bangladesh's Jamuna and Padma basins.

BANGLADESH CONTEXT — FLOOD RISK (AA Framework 2025):
- Monsoon floods occur April–September, peaking June–September.
- In an average year, ~25% of Bangladesh is inundated. Every 4–5 years a severe flood occurs.
- Severe shock threshold: 1-in-5-year return period, affecting 40%+ of population or damaging
  20%+ of household assets.
- Action window: 5–15 days between GloFAS readiness trigger and peak flooding.
- Two basins covered: Jamuna (Bogra, Jamalpur, Gaibandha, Kurigram, Sirajganj) and
  Padma (Madaripur, Manikganj, Shariatpur). 38 Upazilas total.
- Key WASH risk: floodwaters contaminate open water sources, forcing communities to use
  unsafe water → waterborne disease outbreaks. Pumps on low-lying boreholes get submerged.
  Tube wells in flood-prone chars (river islands) and low-lying haors are especially vulnerable.
- Framework WASH actions: pre-position jerricans and water purification tablets (UNICEF),
  deploy mobile water treatment units at evacuation points, community hygiene messaging.
- High-risk locations: chars (river islands), haor areas, low-lying floodplains near the
  Jamuna and Padma rivers, areas with no embankment protection.

VULNERABILITY ASSESSMENT RULES:
- Open/dug wells and surface water: HIGH flood risk (rapid contamination when inundated)
- Tube wells / hand pump boreholes at low elevation: HIGH-MEDIUM risk (submersion of pump
  head, arsenic mobilization after flooding)
- Tube wells on elevated ground or with raised platforms: MEDIUM-LOW risk
- Piped water systems: MEDIUM risk (infrastructure damage, contamination of distribution)
- Non-functional status: escalates risk (cannot be repaired in the 5–15 day action window)
- Old infrastructure (install_year before 2000): higher structural vulnerability
- Remote or char locations (high distance_to_primary): harder to reach for pre-positioning
- Padma basin districts (Madaripur, Manikganj, Shariatpur): newer coverage, potentially
  less pre-positioned supplies than Jamuna basin

Your task: assess ONE waterpoint and return its FLOOD vulnerability rating.

Return ONLY a valid JSON object with this exact structure:
{
  "flood_risk": "High" | "Medium" | "Low",
  "flood_rationale": "1-2 sentence explanation referencing specific attributes and Bangladesh context",
  "priority_action": "One concrete pre-shock anticipatory action for this specific waterpoint"
}

Be decisive. Reference specific attributes AND the district/basin. Apply Bangladesh field knowledge where data is missing.
Do not return anything outside the JSON object."""

# ── Data fetching ─────────────────────────────────────────────────────────────

def fetch_waterpoints(country: str, limit: int) -> pd.DataFrame:
    """Load waterpoints from local CSV file, filtered to framework districts."""
    local_file = "eqje-vguj.csv"
    print(f"\nLoading waterpoints from local file: {local_file}")
    print(f"Target districts: {sorted(FLOOD_DISTRICTS)}")

    if not os.path.exists(local_file):
        raise FileNotFoundError(
            f"Local data file '{local_file}' not found.\n"
            f"Please download it from WPdx+ and place it in the same folder as this script."
        )

    df = pd.read_csv(local_file, low_memory=False)
    print(f"Loaded {len(df)} total waterpoints from file.")

    # Filter to Bangladesh
    if "clean_country_name" in df.columns:
        df = df[df["clean_country_name"] == country].copy()
        print(f"After country filter: {len(df)} waterpoints for {country}")

    # Filter to framework districts using clean_adm2
    if "clean_adm2" in df.columns:
        df["clean_adm2"] = df["clean_adm2"].str.title()
        flood_districts_title = {d.title() for d in FLOOD_DISTRICTS}
        before = len(df)
        df = df[df["clean_adm2"].isin(flood_districts_title)].copy()
        print(f"After district filter: {len(df)} waterpoints "
              f"({before - len(df)} excluded outside framework districts)")
    else:
        print("WARNING: clean_adm2 column not found - processing all rows.")

    if len(df) == 0:
        raise ValueError("No waterpoints found in framework districts. Check your data file.")

    # Cap at requested limit
    df = df.head(limit)
    print(f"Processing {len(df)} waterpoints.")
    return df


def fetch_elevations(df: pd.DataFrame, batch_size: int = 100) -> pd.DataFrame:
    """Fetch elevation for all waterpoints using Open-Meteo Elevation API (free, no key needed).
    Adds elevation_m column to the dataframe."""
    print(f"\nFetching elevations for {len(df)} waterpoints...")
    elevations = []

    for i in range(0, len(df), batch_size):
        batch = df.iloc[i:i + batch_size]
        lats = ",".join(batch["lat_deg"].astype(str).tolist())
        lons = ",".join(batch["lon_deg"].astype(str).tolist())

        try:
            r = requests.get(
                "https://api.open-meteo.com/v1/elevation",
                params={"latitude": lats, "longitude": lons},
                timeout=30
            )
            r.raise_for_status()
            data = r.json()
            elevations.extend(data.get("elevation", [None] * len(batch)))
        except Exception as e:
            print(f"  WARNING: Elevation fetch failed for batch {i//batch_size + 1}: {e}")
            elevations.extend([None] * len(batch))

        time.sleep(1)  # max 1 call/second per API rules

    df = df.copy()
    df["elevation_m"] = elevations
    valid = df["elevation_m"].notna().sum()
    print(f"Elevation fetched for {valid}/{len(df)} waterpoints.")
    if valid > 0:
        print(f"Elevation range: {df['elevation_m'].min():.1f}m – {df['elevation_m'].max():.1f}m")
    return df


def get_frameworks_for_region(region: str) -> list:
    """Return which AA frameworks cover this district (flood phase only)."""
    frameworks = []
    if region in FLOOD_DISTRICTS:
        frameworks.append("flood")
    return frameworks


def get_basin_for_district(district: str) -> str:
    """Return which river basin a district belongs to."""
    if district in JAMUNA_DISTRICTS:
        return "Jamuna"
    elif district in PADMA_DISTRICTS:
        return "Padma"
    return "Unknown"


def prepare_row_for_claude(row: pd.Series) -> dict:
    """Extract and clean vulnerability-relevant attributes from a row."""
    attrs = {}
    for col in VULNERABILITY_COLUMNS:
        val = row.get(col, None)
        if pd.notna(val) and str(val).strip() not in ("", "nan", "None"):
            attrs[col] = str(val).strip()

    # Add location and framework coverage so Claude knows which risks apply
    division = str(row.get("clean_adm1", "Unknown")).strip()
    district = str(row.get("clean_adm2", "Unknown")).strip().title()
    upazila = str(row.get("clean_adm3", "Unknown")).strip()
    frameworks = get_frameworks_for_region(district)

    attrs["division_adm1"] = division
    attrs["district_adm2"] = district
    attrs["upazila_adm3"] = upazila
    attrs["river_basin"] = get_basin_for_district(district)
    attrs["applicable_frameworks"] = frameworks  # always ["flood"] in this phase
    return attrs


# ── Claude classification ─────────────────────────────────────────────────────

def classify_waterpoint(client: anthropic.Anthropic, attributes: dict) -> dict:
    """Call Claude to classify a single waterpoint's vulnerability."""
    
    user_message = f"Waterpoint attributes:\n{json.dumps(attributes, indent=2)}"
    
    response = client.messages.create(
        model="claude-sonnet-4-6",
        max_tokens=400,
        system=SYSTEM_PROMPT,
        messages=[{"role": "user", "content": user_message}]
    )
    
    raw = response.content[0].text.strip()
    
    # Strip markdown code fences if present
    if raw.startswith("```"):
        raw = raw.split("```")[1]
        if raw.startswith("json"):
            raw = raw[4:]
    
    return json.loads(raw)


def classify_with_retry(client: anthropic.Anthropic, attributes: dict, 
                         max_retries: int = 3) -> dict:
    """Classify with retry logic for API errors."""
    for attempt in range(max_retries):
        try:
            return classify_waterpoint(client, attributes)
        except (json.JSONDecodeError, KeyError) as e:
            if attempt == max_retries - 1:
                return {
                    "flood_risk": "Unknown",
                    "flood_rationale": f"Classification failed: {e}",
                    
                    
                    "priority_action": "Manual assessment required"
                }
            time.sleep(1)
        except anthropic.RateLimitError:
            wait = 2 ** attempt * 5
            print(f"\nRate limited. Waiting {wait}s...")
            time.sleep(wait)
        except anthropic.APIError as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(2 ** attempt)


# ── Main pipeline ─────────────────────────────────────────────────────────────

def run_pipeline(country: str, limit: int, output_path: str):
    
    # Init Anthropic client
    api_key = os.environ.get("ANTHROPIC_API_KEY")
    if not api_key:
        raise EnvironmentError("ANTHROPIC_API_KEY environment variable not set.")
    client = anthropic.Anthropic(api_key=api_key)
    
    # Fetch data
    df = fetch_waterpoints(country, limit)
    
    # Fetch elevations for all waterpoints before classification
    df = fetch_elevations(df)

    # Keep only columns that exist in the dataset
    id_cols_present = [c for c in ID_COLUMNS if c in df.columns]
    vuln_cols_present = [c for c in VULNERABILITY_COLUMNS if c in df.columns]
    
    print(f"\nVulnerability columns available in dataset: {vuln_cols_present}")
    print(f"\nStarting classification of {len(df)} waterpoints...")
    print("This may take a few minutes. Each row = 1 API call.\n")
    
    results = []
    
    for _, row in tqdm(df.iterrows(), total=len(df), desc="Classifying"):
        attributes = prepare_row_for_claude(row)
        classification = classify_with_retry(client, attributes)
        
        # Build output row
        out_row = {col: row.get(col, None) for col in id_cols_present}
        out_row["flood_framework"] = "yes"  # all rows here are in flood framework
        out_row.update(classification)
        results.append(out_row)
        
        # Small delay to avoid rate limits
        time.sleep(0.3)
    
    # Save results
    results_df = pd.DataFrame(results)
    
    # Try saving - if file is locked, save to a timestamped backup instead
    import datetime
    try:
        results_df.to_csv(output_path, index=False)
    except PermissionError:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = output_path.replace(".csv", f"_{timestamp}.csv")
        results_df.to_csv(backup_path, index=False)
        print(f"\nWARNING: Could not save to {output_path} (file is open elsewhere)")
        print(f"Results saved to backup file: {backup_path}")
        output_path = backup_path
    
    # Summary
    print(f"\n✅ Done! Results saved to: {output_path}")
    print(f"\nFlood Vulnerability Summary for {country}:")
    print(f"Regions covered: {sorted(FLOOD_REGIONS)}")

    col = "flood_risk"
    if col in results_df.columns:
        counts = results_df[col].value_counts()
        print(f"\n  FLOOD RISK:")
        for level in ["High", "Medium", "Low", "Unknown"]:
            n = counts.get(level, 0)
            pct = n / len(results_df) * 100
            print(f"    {level:8s}: {n:4d} ({pct:.1f}%)")

    print(f"\n  By district:")
    if "clean_adm2" in results_df.columns:
        for district, grp in results_df.groupby("clean_adm2"):
            counts = grp[col].value_counts()
            high = counts.get("High", 0)
            med = counts.get("Medium", 0)
            low = counts.get("Low", 0)
            basin = get_basin_for_district(district)
            print(f"    {district} ({basin}): High={high}, Medium={med}, Low={low}")


# ── Entry point ───────────────────────────────────────────────────────────────

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="WPdx+ Waterpoint Vulnerability Classifier")
    parser.add_argument("--country", default="Bangladesh", 
                        help="Country name as it appears in WPdx+ (default: 'Burkina Faso')")
    parser.add_argument("--limit", type=int, default=500,
                        help="Max waterpoints to process (default: 500)")
    parser.add_argument("--output", default="waterpoint_flood_vulnerability_bangladesh.csv",
                        help="Output CSV filename")
    
    args = parser.parse_args()
    run_pipeline(args.country, args.limit, args.output)
